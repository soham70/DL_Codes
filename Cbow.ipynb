{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d5187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.src.utils import np_utils\n",
    "from keras.preprocessing  import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a37780",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a\n",
    "broader family of machine learning methods based on artificial neural networks\n",
    "with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
    "Deep-learning architectures such as deep neural networks, deep belief networks,\n",
    "deep reinforcement learning, recurrent neural networks, convolutional neural networks and\n",
    "Transformers have been applied to fields including computer vision, speech recognition,\n",
    "natural language processing, machine translation, bioinformatics, drug design,\n",
    "medical image analysis, climate science, material inspection and board game programs,\n",
    "where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "\"\"\"\n",
    "dl_data=data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59226ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocablury_size: 75\n",
      "Vocablury Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer =text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id=tokenizer.word_index\n",
    "\n",
    "word2id['PAD']=0\n",
    "id2word={v:k for k,v in word2id.items()}\n",
    "wids=[[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
    "\n",
    "vocab_size=len(word2id)\n",
    "embed_size=100\n",
    "window_size=2\n",
    "\n",
    "print('Vocablury_size:',vocab_size)\n",
    "print('Vocablury Sample:',list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4bb4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus,window_size,vocab_size):\n",
    "    context_length=window_size*2\n",
    "    \n",
    "    for words in corpus:\n",
    "        sentence_length=len(words)\n",
    "        for index,word in enumerate(words):\n",
    "            context_words=[]\n",
    "            label_words=[]\n",
    "            start=index-window_size\n",
    "            end=index+window_size+1\n",
    "            \n",
    "        context_words.append([\n",
    "            words[i]\n",
    "            for i in range(start,end)\n",
    "            if 0 <= i <sentence_length\n",
    "            and i!=index]\n",
    "        )\n",
    "        \n",
    "    label_words.append(words)\n",
    "    \n",
    "    x=pad_sequences(context_words,maxlen=context_length)\n",
    "    y=np_utils.to_categorical(label_words,vocab_size)\n",
    "    yield(x,y)\n",
    "    \n",
    "i=0\n",
    "for x,y in generate_context_word_pairs(corpus=wids,window_size=window_size,vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "     \n",
    "#       print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    if i==10:\n",
    "        break\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "104bede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 4, 100)            7500      \n",
      "                                                                 \n",
      " lambda_2 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 75)                7575      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15075 (58.89 KB)\n",
      "Trainable params: 15075 (58.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,Lambda\n",
    "\n",
    "cbow=Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size,output_dim=embed_size,input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x,axis=1),output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size,activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
    "\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a19599af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 4.323254108428955\n",
      "Epoch: 2 \tLoss: 4.279575347900391\n",
      "Epoch: 3 \tLoss: 4.246826171875\n",
      "Epoch: 4 \tLoss: 4.218571662902832\n",
      "Epoch: 5 \tLoss: 4.192775249481201\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,6):\n",
    "    loss=0.\n",
    "    i=0\n",
    "    \n",
    "    for x,y in generate_context_word_pairs(corpus=wids,window_size=window_size,vocab_size=vocab_size):\n",
    "        i+=1\n",
    "        loss+=cbow.train_on_batch(x,y)\n",
    "        if i % 100000 ==0:\n",
    "            print('Processed {} (context_words) pairs',format(i))\n",
    "    \n",
    "    print('Epoch:',epoch, '\\tLoss:',loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e149c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>0.027598</td>\n",
       "      <td>0.025616</td>\n",
       "      <td>-0.019534</td>\n",
       "      <td>-0.027853</td>\n",
       "      <td>-0.011096</td>\n",
       "      <td>-0.041121</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>-0.022138</td>\n",
       "      <td>-0.032354</td>\n",
       "      <td>-0.022345</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>-0.045077</td>\n",
       "      <td>-0.042347</td>\n",
       "      <td>0.017261</td>\n",
       "      <td>-0.007474</td>\n",
       "      <td>0.039669</td>\n",
       "      <td>0.029458</td>\n",
       "      <td>0.015225</td>\n",
       "      <td>0.042418</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>-0.005385</td>\n",
       "      <td>-0.012055</td>\n",
       "      <td>-0.024799</td>\n",
       "      <td>0.028562</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.002144</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>-0.037265</td>\n",
       "      <td>-0.022356</td>\n",
       "      <td>0.032604</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>-0.040006</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>-0.020866</td>\n",
       "      <td>0.034473</td>\n",
       "      <td>-0.021448</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>-0.004708</td>\n",
       "      <td>0.008387</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>-0.047634</td>\n",
       "      <td>0.034564</td>\n",
       "      <td>-0.023868</td>\n",
       "      <td>0.039767</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>-0.021325</td>\n",
       "      <td>-0.019272</td>\n",
       "      <td>-0.020100</td>\n",
       "      <td>0.019799</td>\n",
       "      <td>-0.045309</td>\n",
       "      <td>-0.027782</td>\n",
       "      <td>-0.014469</td>\n",
       "      <td>0.011189</td>\n",
       "      <td>-0.018399</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>-0.026299</td>\n",
       "      <td>0.027685</td>\n",
       "      <td>0.013918</td>\n",
       "      <td>0.016753</td>\n",
       "      <td>-0.035251</td>\n",
       "      <td>0.033823</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>-0.046113</td>\n",
       "      <td>0.022424</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.044617</td>\n",
       "      <td>0.037197</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>-0.019985</td>\n",
       "      <td>0.033839</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>-0.022419</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>0.033786</td>\n",
       "      <td>-0.014357</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>0.025523</td>\n",
       "      <td>0.045973</td>\n",
       "      <td>0.014387</td>\n",
       "      <td>-0.003611</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>-0.048849</td>\n",
       "      <td>0.034216</td>\n",
       "      <td>-0.033115</td>\n",
       "      <td>0.025962</td>\n",
       "      <td>0.035385</td>\n",
       "      <td>-0.048719</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>-0.047929</td>\n",
       "      <td>0.045986</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>-0.029819</td>\n",
       "      <td>-0.027500</td>\n",
       "      <td>-0.039103</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>-0.001972</td>\n",
       "      <td>-0.018669</td>\n",
       "      <td>-0.048891</td>\n",
       "      <td>0.005336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>-0.021143</td>\n",
       "      <td>0.034413</td>\n",
       "      <td>0.023518</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>0.029001</td>\n",
       "      <td>-0.040180</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>-0.034120</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>-0.012267</td>\n",
       "      <td>-0.044774</td>\n",
       "      <td>-0.005408</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>-0.026861</td>\n",
       "      <td>0.023174</td>\n",
       "      <td>-0.038788</td>\n",
       "      <td>0.032840</td>\n",
       "      <td>0.043487</td>\n",
       "      <td>-0.043686</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>-0.022286</td>\n",
       "      <td>-0.024876</td>\n",
       "      <td>0.044458</td>\n",
       "      <td>0.023752</td>\n",
       "      <td>-0.018657</td>\n",
       "      <td>-0.006780</td>\n",
       "      <td>-0.035632</td>\n",
       "      <td>-0.004316</td>\n",
       "      <td>-0.023746</td>\n",
       "      <td>-0.010749</td>\n",
       "      <td>-0.003910</td>\n",
       "      <td>0.038316</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>0.009068</td>\n",
       "      <td>0.016674</td>\n",
       "      <td>-0.010182</td>\n",
       "      <td>-0.005328</td>\n",
       "      <td>0.026933</td>\n",
       "      <td>-0.044618</td>\n",
       "      <td>-0.021981</td>\n",
       "      <td>0.019567</td>\n",
       "      <td>0.027864</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>-0.012498</td>\n",
       "      <td>-0.023915</td>\n",
       "      <td>0.034041</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>-0.045906</td>\n",
       "      <td>-0.011711</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>-0.014443</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>0.022856</td>\n",
       "      <td>-0.040289</td>\n",
       "      <td>0.042153</td>\n",
       "      <td>-0.020912</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.046719</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>-0.034905</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>0.024478</td>\n",
       "      <td>0.047617</td>\n",
       "      <td>-0.012916</td>\n",
       "      <td>-0.011070</td>\n",
       "      <td>-0.002731</td>\n",
       "      <td>0.048119</td>\n",
       "      <td>0.022313</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>-0.041510</td>\n",
       "      <td>-0.010531</td>\n",
       "      <td>0.014759</td>\n",
       "      <td>-0.029879</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.037148</td>\n",
       "      <td>0.044667</td>\n",
       "      <td>-0.046681</td>\n",
       "      <td>-0.042880</td>\n",
       "      <td>-0.012753</td>\n",
       "      <td>-0.035796</td>\n",
       "      <td>-0.017842</td>\n",
       "      <td>-0.025125</td>\n",
       "      <td>0.025954</td>\n",
       "      <td>0.047032</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>-0.018513</td>\n",
       "      <td>-0.012364</td>\n",
       "      <td>-0.041676</td>\n",
       "      <td>0.005962</td>\n",
       "      <td>-0.011412</td>\n",
       "      <td>-0.034275</td>\n",
       "      <td>0.034940</td>\n",
       "      <td>-0.020806</td>\n",
       "      <td>0.039420</td>\n",
       "      <td>-0.041171</td>\n",
       "      <td>0.019655</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>-0.006308</td>\n",
       "      <td>0.048166</td>\n",
       "      <td>-0.049243</td>\n",
       "      <td>-0.026706</td>\n",
       "      <td>-0.025683</td>\n",
       "      <td>-0.008417</td>\n",
       "      <td>-0.041196</td>\n",
       "      <td>0.039105</td>\n",
       "      <td>0.042512</td>\n",
       "      <td>0.026013</td>\n",
       "      <td>-0.031120</td>\n",
       "      <td>0.049220</td>\n",
       "      <td>-0.009368</td>\n",
       "      <td>0.010416</td>\n",
       "      <td>-0.036460</td>\n",
       "      <td>-0.008143</td>\n",
       "      <td>-0.022974</td>\n",
       "      <td>-0.038073</td>\n",
       "      <td>-0.045128</td>\n",
       "      <td>-0.039188</td>\n",
       "      <td>0.006305</td>\n",
       "      <td>0.047492</td>\n",
       "      <td>-0.014406</td>\n",
       "      <td>0.016801</td>\n",
       "      <td>-0.022442</td>\n",
       "      <td>-0.032547</td>\n",
       "      <td>0.009190</td>\n",
       "      <td>-0.021149</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>0.020178</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>-0.011128</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>0.049293</td>\n",
       "      <td>0.048292</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>-0.037697</td>\n",
       "      <td>0.031497</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>0.042310</td>\n",
       "      <td>0.018364</td>\n",
       "      <td>-0.033493</td>\n",
       "      <td>-0.043700</td>\n",
       "      <td>-0.014271</td>\n",
       "      <td>-0.000804</td>\n",
       "      <td>-0.008042</td>\n",
       "      <td>-0.026013</td>\n",
       "      <td>-0.009877</td>\n",
       "      <td>-0.038834</td>\n",
       "      <td>0.041119</td>\n",
       "      <td>0.044256</td>\n",
       "      <td>-0.044802</td>\n",
       "      <td>-0.006160</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>-0.043810</td>\n",
       "      <td>-0.011931</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>-0.030288</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>0.041038</td>\n",
       "      <td>-0.011373</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>-0.048982</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>-0.041228</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>-0.034774</td>\n",
       "      <td>-0.038031</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>0.011007</td>\n",
       "      <td>-0.040250</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>-0.024180</td>\n",
       "      <td>0.038996</td>\n",
       "      <td>-0.033406</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>-0.046947</td>\n",
       "      <td>-0.036140</td>\n",
       "      <td>-0.008591</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>-0.012003</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>-0.005630</td>\n",
       "      <td>0.038407</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-0.012470</td>\n",
       "      <td>-0.031172</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.020743</td>\n",
       "      <td>-0.048718</td>\n",
       "      <td>-0.008010</td>\n",
       "      <td>-0.002326</td>\n",
       "      <td>0.045148</td>\n",
       "      <td>-0.035585</td>\n",
       "      <td>0.006936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.033406</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.034437</td>\n",
       "      <td>0.034295</td>\n",
       "      <td>0.037554</td>\n",
       "      <td>-0.024445</td>\n",
       "      <td>0.034618</td>\n",
       "      <td>0.014313</td>\n",
       "      <td>0.037232</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.043804</td>\n",
       "      <td>-0.033539</td>\n",
       "      <td>0.017179</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>0.033640</td>\n",
       "      <td>0.041534</td>\n",
       "      <td>0.028493</td>\n",
       "      <td>-0.008497</td>\n",
       "      <td>0.017475</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>0.032182</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>-0.033022</td>\n",
       "      <td>0.018197</td>\n",
       "      <td>0.034944</td>\n",
       "      <td>-0.005193</td>\n",
       "      <td>-0.012367</td>\n",
       "      <td>-0.010196</td>\n",
       "      <td>-0.012087</td>\n",
       "      <td>0.043449</td>\n",
       "      <td>-0.045064</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.010337</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>-0.018010</td>\n",
       "      <td>0.034441</td>\n",
       "      <td>-0.022676</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>0.028924</td>\n",
       "      <td>0.037757</td>\n",
       "      <td>0.043106</td>\n",
       "      <td>0.012056</td>\n",
       "      <td>0.043003</td>\n",
       "      <td>0.049594</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>0.048960</td>\n",
       "      <td>0.043888</td>\n",
       "      <td>0.016139</td>\n",
       "      <td>0.025027</td>\n",
       "      <td>-0.002773</td>\n",
       "      <td>-0.041604</td>\n",
       "      <td>-0.047850</td>\n",
       "      <td>0.017779</td>\n",
       "      <td>0.049163</td>\n",
       "      <td>-0.021325</td>\n",
       "      <td>-0.021487</td>\n",
       "      <td>-0.032288</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>-0.017102</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>-0.010501</td>\n",
       "      <td>0.039564</td>\n",
       "      <td>0.013987</td>\n",
       "      <td>0.017321</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>-0.041007</td>\n",
       "      <td>0.037816</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>-0.008848</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>-0.006911</td>\n",
       "      <td>-0.017929</td>\n",
       "      <td>0.038528</td>\n",
       "      <td>-0.004046</td>\n",
       "      <td>-0.022257</td>\n",
       "      <td>-0.043152</td>\n",
       "      <td>-0.021433</td>\n",
       "      <td>-0.029238</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>-0.025428</td>\n",
       "      <td>0.042561</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>-0.007400</td>\n",
       "      <td>-0.004099</td>\n",
       "      <td>-0.043067</td>\n",
       "      <td>-0.017311</td>\n",
       "      <td>0.014901</td>\n",
       "      <td>-0.029218</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.039222</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>0.040217</td>\n",
       "      <td>0.047090</td>\n",
       "      <td>0.003496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.026456</td>\n",
       "      <td>-0.009380</td>\n",
       "      <td>-0.007531</td>\n",
       "      <td>0.049292</td>\n",
       "      <td>0.028448</td>\n",
       "      <td>0.025297</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>-0.034378</td>\n",
       "      <td>0.012799</td>\n",
       "      <td>-0.012042</td>\n",
       "      <td>0.031363</td>\n",
       "      <td>-0.020835</td>\n",
       "      <td>-0.033664</td>\n",
       "      <td>0.024213</td>\n",
       "      <td>0.027116</td>\n",
       "      <td>-0.041964</td>\n",
       "      <td>-0.035758</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>0.041815</td>\n",
       "      <td>-0.002341</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>0.038108</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.037158</td>\n",
       "      <td>0.043180</td>\n",
       "      <td>-0.013027</td>\n",
       "      <td>0.031433</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.017673</td>\n",
       "      <td>-0.034465</td>\n",
       "      <td>-0.021738</td>\n",
       "      <td>0.037242</td>\n",
       "      <td>-0.042194</td>\n",
       "      <td>-0.034069</td>\n",
       "      <td>0.048242</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.021714</td>\n",
       "      <td>0.030948</td>\n",
       "      <td>0.024980</td>\n",
       "      <td>-0.030603</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>-0.043710</td>\n",
       "      <td>0.020196</td>\n",
       "      <td>-0.011339</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>-0.010103</td>\n",
       "      <td>-0.032175</td>\n",
       "      <td>0.044398</td>\n",
       "      <td>-0.040960</td>\n",
       "      <td>0.014130</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>-0.002169</td>\n",
       "      <td>-0.046147</td>\n",
       "      <td>0.027577</td>\n",
       "      <td>-0.028249</td>\n",
       "      <td>-0.042248</td>\n",
       "      <td>-0.039442</td>\n",
       "      <td>0.030436</td>\n",
       "      <td>0.035144</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.044811</td>\n",
       "      <td>-0.049434</td>\n",
       "      <td>-0.049056</td>\n",
       "      <td>0.037574</td>\n",
       "      <td>0.031684</td>\n",
       "      <td>-0.033714</td>\n",
       "      <td>-0.005920</td>\n",
       "      <td>0.024901</td>\n",
       "      <td>0.014483</td>\n",
       "      <td>-0.041641</td>\n",
       "      <td>-0.049367</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>-0.024374</td>\n",
       "      <td>-0.033999</td>\n",
       "      <td>0.029219</td>\n",
       "      <td>0.034063</td>\n",
       "      <td>0.037006</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>0.030278</td>\n",
       "      <td>0.015962</td>\n",
       "      <td>-0.012003</td>\n",
       "      <td>-0.039093</td>\n",
       "      <td>0.038928</td>\n",
       "      <td>-0.037269</td>\n",
       "      <td>0.022601</td>\n",
       "      <td>-0.036158</td>\n",
       "      <td>0.040125</td>\n",
       "      <td>0.020603</td>\n",
       "      <td>0.047660</td>\n",
       "      <td>-0.024149</td>\n",
       "      <td>-0.018883</td>\n",
       "      <td>-0.008196</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.041437</td>\n",
       "      <td>-0.038135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6   ...        93        94        95        96        97        98        99\n",
       "deep      0.027598  0.025616 -0.019534 -0.027853 -0.011096 -0.041121  0.016038  ... -0.027500 -0.039103  0.020287 -0.001972 -0.018669 -0.048891  0.005336\n",
       "networks -0.021143  0.034413  0.023518  0.018158  0.029001 -0.040180  0.008220  ... -0.034275  0.034940 -0.020806  0.039420 -0.041171  0.019655  0.049100\n",
       "neural   -0.006308  0.048166 -0.049243 -0.026706 -0.025683 -0.008417 -0.041196  ...  0.020743 -0.048718 -0.008010 -0.002326  0.045148 -0.035585  0.006936\n",
       "and       0.033406  0.007621 -0.033398  0.034437  0.034295  0.037554 -0.024445  ... -0.029218  0.008923  0.039222  0.009595  0.040217  0.047090  0.003496\n",
       "as        0.026456 -0.009380 -0.007531  0.049292  0.028448  0.025297  0.010735  ...  0.047660 -0.024149 -0.018883 -0.008196  0.034286  0.041437 -0.038135\n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=cbow.get_weights()[0]\n",
    "weights=weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights,index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b7c289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 74)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deep': ['vision', 'programs', 'broader', 'known', 'family']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distance_matrix=euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similiar_wprds={search_term :[id2word[idx]for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n",
    "               for search_term in ['deep']\n",
    "               }\n",
    "similiar_wprds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
