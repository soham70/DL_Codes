{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5bc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.src.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633d0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a\n",
    "broader family of machine learning methods based on artificial neural networks\n",
    "with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
    "Deep-learning architectures such as deep neural networks, deep belief networks,\n",
    "deep reinforcement learning, recurrent neural networks, convolutional neural networks and\n",
    "Transformers have been applied to fields including computer vision, speech recognition,\n",
    "natural language processing, machine translation, bioinformatics, drug design,\n",
    "medical image analysis, climate science, material inspection and board game programs,\n",
    "where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "\"\"\"\n",
    "\n",
    "dl_data=data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41966668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vovablury Size:  75\n",
      "Vocablury Sample:  [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer =text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id=tokenizer.word_index\n",
    "\n",
    "word2id['PAD']=0\n",
    "id2word={v:k for k,v in word2id.items()}\n",
    "wids=[[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
    "         \n",
    "vocab_size=len(word2id)\n",
    "embed_size=100\n",
    "window_size=2\n",
    "\n",
    "print('Vovablury Size: ',vocab_size)\n",
    "print('Vocablury Sample: ',list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea0291f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_words(corpus,window_size,vocab_size):\n",
    "    context_length=window_size*2\n",
    "    \n",
    "    for words in corpus:\n",
    "        sentence_length=len(words)\n",
    "        for word,index in enumerate(words):\n",
    "            context_words=[]\n",
    "            label_words=[]\n",
    "            start=index-window_size\n",
    "            end=index+window_size+1\n",
    "            context_words.append([\n",
    "                words[i]\n",
    "                for i in range(start,end)\n",
    "                if 0 <= i <sentence_length\n",
    "                and i!=index\n",
    "]            )\n",
    "            label_words.append(words)\n",
    "\n",
    "            x=pad_sequences(context_words,maxlen=context_length)\n",
    "            y=np_utils.to_categorical(label_words,vocab_size)\n",
    "            yield(x,y)\n",
    "\n",
    "i=0\n",
    "\n",
    "for x,y in generate_context_words(corpus=wids,window_size=window_size,vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        \n",
    "        if i==10:\n",
    "            break\n",
    "        i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70d05bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 4, 100)            7500      \n",
      "                                                                 \n",
      " lambda_2 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 75)                7575      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15075 (58.89 KB)\n",
      "Trainable params: 15075 (58.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Embedding,Lambda\n",
    "\n",
    "cbow=Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size,output_dim=embed_size,input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x,axis=1),output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size,activation='relu'))\n",
    "cbow.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
    "\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "574ec799",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node categorical_crossentropy/cond/remove_squeezable_dimensions/cond/Squeeze defined at (most recent call last):\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n\n  File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_21444\\3055759913.py\", line 7, in <cell line: 1>\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2763, in train_on_batch\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 263, in call\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 209, in squeeze_or_expand_dimensions\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 204, in <lambda>\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 155, in remove_squeezable_dimensions\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 157, in <lambda>\n\nCan not squeeze dim[2], expected a dimension of 1, got 75\n\t [[{{node categorical_crossentropy/cond/remove_squeezable_dimensions/cond/Squeeze}}]] [Op:__inference_train_function_1293]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m generate_context_words(corpus\u001b[38;5;241m=\u001b[39mwids,window_size\u001b[38;5;241m=\u001b[39mwindow_size,vocab_size\u001b[38;5;241m=\u001b[39mvocab_size):\n\u001b[0;32m      6\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mcbow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100000\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (context_word) pairs\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mformat\u001b[39m(i))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:2763\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2759\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2760\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2761\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2763\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2765\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node categorical_crossentropy/cond/remove_squeezable_dimensions/cond/Squeeze defined at (most recent call last):\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n\n  File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_21444\\3055759913.py\", line 7, in <cell line: 1>\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2763, in train_on_batch\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 263, in call\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 209, in squeeze_or_expand_dimensions\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 204, in <lambda>\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 155, in remove_squeezable_dimensions\n\n  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 157, in <lambda>\n\nCan not squeeze dim[2], expected a dimension of 1, got 75\n\t [[{{node categorical_crossentropy/cond/remove_squeezable_dimensions/cond/Squeeze}}]] [Op:__inference_train_function_1293]"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,6):\n",
    "    loss=0.\n",
    "    i=0\n",
    "    \n",
    "    for x, y in generate_context_words(corpus=wids,window_size=window_size,vocab_size=vocab_size):\n",
    "        i+=1\n",
    "        loss+=cbow.train_on_batch(x,y)\n",
    "        \n",
    "        if i % 100000 ==0:\n",
    "            print('Processed {} (context_word) pairs',format(i))\n",
    "    print('Epoch:',epoch,'\\tLoss:',loss)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21d57755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>-0.019836</td>\n",
       "      <td>0.035901</td>\n",
       "      <td>-0.037689</td>\n",
       "      <td>-0.043634</td>\n",
       "      <td>-0.003655</td>\n",
       "      <td>0.038840</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>-0.013693</td>\n",
       "      <td>-0.010249</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.043523</td>\n",
       "      <td>-0.049723</td>\n",
       "      <td>0.025804</td>\n",
       "      <td>0.042881</td>\n",
       "      <td>0.032975</td>\n",
       "      <td>-0.022084</td>\n",
       "      <td>0.033041</td>\n",
       "      <td>-0.003166</td>\n",
       "      <td>0.030577</td>\n",
       "      <td>-0.009308</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>0.018277</td>\n",
       "      <td>0.024595</td>\n",
       "      <td>-0.041799</td>\n",
       "      <td>-0.008205</td>\n",
       "      <td>-0.009811</td>\n",
       "      <td>-0.045999</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>-0.048329</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.034637</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.039431</td>\n",
       "      <td>-0.034458</td>\n",
       "      <td>-0.039591</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>0.035324</td>\n",
       "      <td>0.040022</td>\n",
       "      <td>0.018841</td>\n",
       "      <td>-0.049872</td>\n",
       "      <td>0.038758</td>\n",
       "      <td>-0.037766</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.034356</td>\n",
       "      <td>-0.002149</td>\n",
       "      <td>-0.040055</td>\n",
       "      <td>-0.048888</td>\n",
       "      <td>-0.049978</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>-0.018569</td>\n",
       "      <td>-0.028933</td>\n",
       "      <td>0.044266</td>\n",
       "      <td>-0.028960</td>\n",
       "      <td>0.047482</td>\n",
       "      <td>-0.028468</td>\n",
       "      <td>-0.048813</td>\n",
       "      <td>0.044663</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>-0.045253</td>\n",
       "      <td>-0.049627</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>-0.048894</td>\n",
       "      <td>-0.016077</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>0.031659</td>\n",
       "      <td>0.023376</td>\n",
       "      <td>-0.003065</td>\n",
       "      <td>0.041534</td>\n",
       "      <td>-0.040173</td>\n",
       "      <td>0.011520</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>-0.016647</td>\n",
       "      <td>0.035602</td>\n",
       "      <td>0.037392</td>\n",
       "      <td>-0.016338</td>\n",
       "      <td>-0.046125</td>\n",
       "      <td>0.030527</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>-0.040301</td>\n",
       "      <td>0.045313</td>\n",
       "      <td>-0.021557</td>\n",
       "      <td>0.026521</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>0.034810</td>\n",
       "      <td>-0.015468</td>\n",
       "      <td>-0.004885</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>-0.049026</td>\n",
       "      <td>-0.037189</td>\n",
       "      <td>-0.035134</td>\n",
       "      <td>-0.017806</td>\n",
       "      <td>0.041377</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>-0.035659</td>\n",
       "      <td>0.028839</td>\n",
       "      <td>-0.030406</td>\n",
       "      <td>-0.025793</td>\n",
       "      <td>0.018365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.049594</td>\n",
       "      <td>-0.028568</td>\n",
       "      <td>-0.042277</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-0.051359</td>\n",
       "      <td>0.047919</td>\n",
       "      <td>0.016030</td>\n",
       "      <td>-0.028038</td>\n",
       "      <td>-0.012239</td>\n",
       "      <td>0.022803</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>-0.003842</td>\n",
       "      <td>-0.056391</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>-0.002506</td>\n",
       "      <td>-0.032568</td>\n",
       "      <td>-0.033393</td>\n",
       "      <td>0.059245</td>\n",
       "      <td>0.057607</td>\n",
       "      <td>0.022649</td>\n",
       "      <td>0.015031</td>\n",
       "      <td>-0.050173</td>\n",
       "      <td>-0.015436</td>\n",
       "      <td>0.063541</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>-0.018873</td>\n",
       "      <td>-0.024845</td>\n",
       "      <td>0.021696</td>\n",
       "      <td>-0.021525</td>\n",
       "      <td>-0.023892</td>\n",
       "      <td>-0.013428</td>\n",
       "      <td>-0.028730</td>\n",
       "      <td>0.024717</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>-0.052787</td>\n",
       "      <td>-0.011139</td>\n",
       "      <td>-0.004403</td>\n",
       "      <td>0.016791</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>0.013332</td>\n",
       "      <td>0.007147</td>\n",
       "      <td>-0.025811</td>\n",
       "      <td>-0.038990</td>\n",
       "      <td>-0.056724</td>\n",
       "      <td>0.040159</td>\n",
       "      <td>-0.051886</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>-0.044599</td>\n",
       "      <td>-0.065946</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.039436</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>-0.045387</td>\n",
       "      <td>-0.031101</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.044277</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>-0.025416</td>\n",
       "      <td>-0.011625</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>-0.048548</td>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.020879</td>\n",
       "      <td>-0.041625</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>-0.064158</td>\n",
       "      <td>0.038533</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>-0.046571</td>\n",
       "      <td>-0.059844</td>\n",
       "      <td>-0.021362</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>0.037217</td>\n",
       "      <td>-0.007762</td>\n",
       "      <td>-0.006987</td>\n",
       "      <td>-0.019348</td>\n",
       "      <td>0.034827</td>\n",
       "      <td>-0.068607</td>\n",
       "      <td>-0.006009</td>\n",
       "      <td>0.052724</td>\n",
       "      <td>-0.017666</td>\n",
       "      <td>0.027001</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>-0.008179</td>\n",
       "      <td>-0.057694</td>\n",
       "      <td>0.039730</td>\n",
       "      <td>0.025904</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>-0.033238</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>-0.007707</td>\n",
       "      <td>0.036942</td>\n",
       "      <td>0.037498</td>\n",
       "      <td>0.030019</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>-0.021899</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>-0.042821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.002713</td>\n",
       "      <td>-0.032696</td>\n",
       "      <td>0.039049</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.032703</td>\n",
       "      <td>-0.002355</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>-0.027544</td>\n",
       "      <td>-0.022652</td>\n",
       "      <td>-0.039298</td>\n",
       "      <td>0.021408</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>-0.040411</td>\n",
       "      <td>-0.027999</td>\n",
       "      <td>-0.048090</td>\n",
       "      <td>0.044496</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>-0.027806</td>\n",
       "      <td>-0.003458</td>\n",
       "      <td>-0.036425</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.012877</td>\n",
       "      <td>-0.028299</td>\n",
       "      <td>0.028496</td>\n",
       "      <td>-0.017757</td>\n",
       "      <td>-0.021154</td>\n",
       "      <td>0.048874</td>\n",
       "      <td>-0.008383</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.049670</td>\n",
       "      <td>0.039892</td>\n",
       "      <td>-0.041856</td>\n",
       "      <td>0.046396</td>\n",
       "      <td>0.040845</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>-0.020222</td>\n",
       "      <td>-0.046909</td>\n",
       "      <td>-0.015277</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>0.015738</td>\n",
       "      <td>0.046009</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>0.043899</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>-0.035550</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>-0.015486</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>0.039896</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>-0.005839</td>\n",
       "      <td>-0.009616</td>\n",
       "      <td>-0.044900</td>\n",
       "      <td>-0.025565</td>\n",
       "      <td>0.020509</td>\n",
       "      <td>0.045350</td>\n",
       "      <td>0.036147</td>\n",
       "      <td>-0.008209</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>0.024765</td>\n",
       "      <td>-0.042853</td>\n",
       "      <td>-0.015782</td>\n",
       "      <td>-0.021512</td>\n",
       "      <td>0.047695</td>\n",
       "      <td>-0.047484</td>\n",
       "      <td>0.048873</td>\n",
       "      <td>-0.037857</td>\n",
       "      <td>-0.018315</td>\n",
       "      <td>0.018537</td>\n",
       "      <td>0.048381</td>\n",
       "      <td>-0.028365</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.019258</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>0.014591</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>-0.027527</td>\n",
       "      <td>0.009591</td>\n",
       "      <td>0.044377</td>\n",
       "      <td>-0.008335</td>\n",
       "      <td>0.031633</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-0.006781</td>\n",
       "      <td>0.036851</td>\n",
       "      <td>-0.042338</td>\n",
       "      <td>-0.037236</td>\n",
       "      <td>-0.019051</td>\n",
       "      <td>-0.023430</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>-0.033484</td>\n",
       "      <td>-0.045682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.012596</td>\n",
       "      <td>0.024340</td>\n",
       "      <td>0.031935</td>\n",
       "      <td>-0.009512</td>\n",
       "      <td>-0.040552</td>\n",
       "      <td>0.042698</td>\n",
       "      <td>0.023980</td>\n",
       "      <td>-0.033016</td>\n",
       "      <td>-0.044623</td>\n",
       "      <td>0.028805</td>\n",
       "      <td>-0.000516</td>\n",
       "      <td>0.026436</td>\n",
       "      <td>-0.046534</td>\n",
       "      <td>-0.023277</td>\n",
       "      <td>-0.049617</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>0.005645</td>\n",
       "      <td>0.049066</td>\n",
       "      <td>-0.006778</td>\n",
       "      <td>-0.037316</td>\n",
       "      <td>0.038057</td>\n",
       "      <td>0.032607</td>\n",
       "      <td>-0.001337</td>\n",
       "      <td>0.045195</td>\n",
       "      <td>0.044072</td>\n",
       "      <td>0.013456</td>\n",
       "      <td>-0.008257</td>\n",
       "      <td>-0.001914</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>-0.006449</td>\n",
       "      <td>0.035152</td>\n",
       "      <td>0.037239</td>\n",
       "      <td>-0.007844</td>\n",
       "      <td>0.044627</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>0.045143</td>\n",
       "      <td>-0.005084</td>\n",
       "      <td>-0.040030</td>\n",
       "      <td>-0.001087</td>\n",
       "      <td>-0.031373</td>\n",
       "      <td>0.013372</td>\n",
       "      <td>0.032305</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>-0.004280</td>\n",
       "      <td>-0.033212</td>\n",
       "      <td>-0.023419</td>\n",
       "      <td>-0.006482</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>-0.032961</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>-0.005653</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>-0.020265</td>\n",
       "      <td>-0.032478</td>\n",
       "      <td>-0.048869</td>\n",
       "      <td>0.045396</td>\n",
       "      <td>-0.019904</td>\n",
       "      <td>0.019718</td>\n",
       "      <td>-0.037766</td>\n",
       "      <td>0.018224</td>\n",
       "      <td>0.043657</td>\n",
       "      <td>0.039685</td>\n",
       "      <td>-0.001934</td>\n",
       "      <td>-0.034051</td>\n",
       "      <td>0.028093</td>\n",
       "      <td>-0.005655</td>\n",
       "      <td>0.045471</td>\n",
       "      <td>-0.040398</td>\n",
       "      <td>0.026695</td>\n",
       "      <td>-0.020202</td>\n",
       "      <td>-0.037254</td>\n",
       "      <td>-0.045913</td>\n",
       "      <td>0.047823</td>\n",
       "      <td>-0.026130</td>\n",
       "      <td>0.018752</td>\n",
       "      <td>0.031545</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>-0.009017</td>\n",
       "      <td>-0.016945</td>\n",
       "      <td>-0.028600</td>\n",
       "      <td>-0.007899</td>\n",
       "      <td>-0.030165</td>\n",
       "      <td>0.033877</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>-0.042876</td>\n",
       "      <td>0.046135</td>\n",
       "      <td>0.009899</td>\n",
       "      <td>-0.024637</td>\n",
       "      <td>0.027692</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>-0.027279</td>\n",
       "      <td>0.046649</td>\n",
       "      <td>-0.018214</td>\n",
       "      <td>0.030586</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>-0.002029</td>\n",
       "      <td>0.030041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.036352</td>\n",
       "      <td>-0.012028</td>\n",
       "      <td>-0.011811</td>\n",
       "      <td>0.022789</td>\n",
       "      <td>0.027550</td>\n",
       "      <td>-0.035723</td>\n",
       "      <td>-0.023171</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.031478</td>\n",
       "      <td>0.044546</td>\n",
       "      <td>-0.013018</td>\n",
       "      <td>0.035130</td>\n",
       "      <td>0.029794</td>\n",
       "      <td>-0.045795</td>\n",
       "      <td>-0.015462</td>\n",
       "      <td>-0.033130</td>\n",
       "      <td>-0.043919</td>\n",
       "      <td>-0.028301</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>-0.031995</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>0.040708</td>\n",
       "      <td>-0.005505</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>-0.044473</td>\n",
       "      <td>-0.037080</td>\n",
       "      <td>-0.046647</td>\n",
       "      <td>-0.009930</td>\n",
       "      <td>0.028568</td>\n",
       "      <td>0.038796</td>\n",
       "      <td>-0.044831</td>\n",
       "      <td>-0.031606</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>-0.019183</td>\n",
       "      <td>-0.035083</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>0.036109</td>\n",
       "      <td>0.009713</td>\n",
       "      <td>0.016128</td>\n",
       "      <td>-0.028429</td>\n",
       "      <td>-0.028146</td>\n",
       "      <td>-0.034147</td>\n",
       "      <td>-0.023825</td>\n",
       "      <td>-0.006465</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.015696</td>\n",
       "      <td>-0.021688</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>-0.001289</td>\n",
       "      <td>0.022878</td>\n",
       "      <td>-0.018367</td>\n",
       "      <td>-0.003684</td>\n",
       "      <td>-0.016164</td>\n",
       "      <td>-0.042252</td>\n",
       "      <td>-0.004893</td>\n",
       "      <td>-0.026732</td>\n",
       "      <td>-0.029660</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>-0.007862</td>\n",
       "      <td>-0.021772</td>\n",
       "      <td>-0.011312</td>\n",
       "      <td>-0.040107</td>\n",
       "      <td>0.045146</td>\n",
       "      <td>-0.001495</td>\n",
       "      <td>-0.017547</td>\n",
       "      <td>-0.026699</td>\n",
       "      <td>-0.034726</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.028771</td>\n",
       "      <td>-0.023828</td>\n",
       "      <td>-0.002326</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>-0.017407</td>\n",
       "      <td>0.033132</td>\n",
       "      <td>0.017024</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>-0.032275</td>\n",
       "      <td>0.036595</td>\n",
       "      <td>-0.044840</td>\n",
       "      <td>-0.030885</td>\n",
       "      <td>0.023727</td>\n",
       "      <td>-0.018164</td>\n",
       "      <td>-0.041512</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>0.034046</td>\n",
       "      <td>0.044578</td>\n",
       "      <td>-0.049485</td>\n",
       "      <td>0.045974</td>\n",
       "      <td>0.032970</td>\n",
       "      <td>-0.004853</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.017504</td>\n",
       "      <td>-0.013580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6   ...        93        94        95        96        97        98        99\n",
       "deep     -0.019836  0.035901 -0.037689 -0.043634 -0.003655  0.038840  0.027283  ...  0.041377  0.049775 -0.035659  0.028839 -0.030406 -0.025793  0.018365\n",
       "networks  0.005720  0.016231  0.049594 -0.028568 -0.042277  0.000279 -0.051359  ...  0.036942  0.037498  0.030019  0.016355 -0.021899 -0.028389 -0.042821\n",
       "neural    0.002713 -0.032696  0.039049  0.039100  0.013145  0.032703 -0.002355  ... -0.042338 -0.037236 -0.019051 -0.023430 -0.000844 -0.033484 -0.045682\n",
       "and      -0.012596  0.024340  0.031935 -0.009512 -0.040552  0.042698  0.023980  ... -0.027279  0.046649 -0.018214  0.030586  0.044670 -0.002029  0.030041\n",
       "as        0.036352 -0.012028 -0.011811  0.022789  0.027550 -0.035723 -0.023171  ... -0.049485  0.045974  0.032970 -0.004853  0.011186  0.017504 -0.013580\n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=cbow.get_weights()[0]\n",
    "weights=weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights,index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d358054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 74)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deep': ['belief', 'have', 'family', 'inspection', 'semi']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distance_matrix=euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similiar_words={search_term:[id2word[idx]for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n",
    "               for search_term in ['deep']\n",
    "               }\n",
    "\n",
    "similiar_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
